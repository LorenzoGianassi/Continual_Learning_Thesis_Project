{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tesi_definitivo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/l9sKPzOpBi3B6Q93Jdvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoGianassi/Continual_Learning_Thesis/blob/main/Tesi_definitivo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVFAYYRajYhj"
      },
      "source": [
        "\n",
        "from itertools import islice\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.optim import SGD\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import MNIST\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from timeit import default_timer as timer\n",
        "from torch.utils.data import Subset\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import copy\n",
        "from torch.backends import cudnn\n",
        "import pandas as pd\n",
        "\n",
        "# associo cuda per lavorare ulla gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)\n",
        "                #####################################################################################\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "\n",
        "                #####################################################################################\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self,numFC2=84):\n",
        "      super(Net, self).__init__()\n",
        "      self.numFC2 = numFC2\n",
        "      self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "      self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "      self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "      self.conv5 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "      self.conv6 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "      self.fc1 = nn.Linear(256*8*8, 120)\n",
        "      self.fc2 = nn.Linear(120, numFC2)\n",
        "      self.task_fcs = []       # Will hold all Linear layers for classification heads.\n",
        "      self.current_tasks = []  # Selects which task(s) are currently active.\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x =(F.relu(self.conv1(x)))\n",
        "    x =(F.relu(self.conv2(x)))\n",
        "    x =(F.relu(self.conv3(x)))\n",
        "    x = self.pool(x)\n",
        "    x =(F.relu(self.conv4(x)))\n",
        "    x =(F.relu(self.conv5(x)))\n",
        "    x =(F.relu(self.conv6(x)))\n",
        "    x = self.pool(x)\n",
        "    x = x.view(-1, 256*8*8)\n",
        "    x = self.dropout(F.relu(self.fc1(x)))\n",
        "    x = self.dropout (F.relu(self.fc2(x)))\n",
        "    # Concatenates the classification heads of selected tasks.\n",
        "    outputs = torch.cat([(self.task_fcs[t](x)) for t in self.current_tasks], 1)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  # Add a new classification head with num_classes outputs.\n",
        "  def add_task(self, num_classes):\n",
        "      fc = nn.Linear(self.numFC2, num_classes)\n",
        "      self.add_module(name=f'task{len(self.task_fcs)}_fc', module=fc)\n",
        "      #self.add_module(name=str(len(self.task_fcs)), module=fc)\n",
        "      self.task_fcs.append(fc)\n",
        "\n",
        "  # Set the current task(s) -- takes a *LIST* of task ids.\n",
        "  def set_tasks(self, tasks):\n",
        "      self.current_tasks = tasks\n",
        "\n",
        "\n",
        "                                         ###########################################################################\n",
        "\n",
        "# CLASSE IL DATASET FILTRATO\n",
        "class Filtered_Dataset(Dataset): \n",
        "  def __init__(self, dataset, indices, offset=0):\n",
        "        self.indices = indices # indici dei target di cui ci interessa il target\n",
        "        self.original_indices = [i for i in range(len(dataset.targets)) if dataset.targets[i] in indices] # filtraggio degli indici del dataset che voglio nel subset\n",
        "        self.dataset = Subset(dataset, self.original_indices)\n",
        "        #self.original2task = { indices[i]  : offset + i for i in range(0,  len(indices) ) } # remapping degli indici con chiavi i valori degli indici del del task\n",
        "        self.original2task = self.dictmap(indices,offset)\n",
        "        self.task2original = dict({ (val, key) for (key, val) in self.original2task.items() }) # remapping degli indici con chiavi i valori degli indici del del task\n",
        "        \n",
        "  def __getitem__(self, idx):\n",
        "        (x, y) = self.dataset[idx]\n",
        "        return (x, self.original2task[y])\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.original_indices)\n",
        "\n",
        "  def dictmap(self,indices,offset):\n",
        "      original2task={}\n",
        "      idx = np.arange(len(indices))\n",
        "      idx_tmp = np.arange(len(indices))\n",
        "      for i in range(0,len(idx)):\n",
        "        if (idx[i]+offset) in indices:\n",
        "          original2task[idx[i]+offset] = (idx[i] + offset)\n",
        "          indices  = np.delete(indices, np.argwhere(indices == (idx[i]+offset)))\n",
        "          idx_tmp  = np.delete(idx_tmp,np.argwhere(idx_tmp == (idx[i])))     \n",
        "      for i in range(0,len(indices)):\n",
        "          original2task[indices[i]]=idx_tmp[i] + offset   \n",
        "      return original2task\n",
        "\n",
        "\n",
        "                                          ###########################################################################\n",
        "\n",
        "# METODO PER SPLITTARE GLI INDICI DEI TARGET DEL TRAINSET/TESTSET\n",
        "def idx_tasks(num_tasks,trainset,rand=True):   # genero un np array e splitto in task\n",
        "  targets = np.unique(trainset.targets)\n",
        "  tasks = []\n",
        "  if rand:\n",
        "    unique_targets = np.random.permutation(len(targets))\n",
        "    tasks = np.split(unique_targets,num_tasks,axis=0)\n",
        "  else:  \n",
        "    tasks = np.split(targets,num_tasks,axis=0)\n",
        "  if num_tasks!=1 and rand:  \n",
        "    for i in range(0,len(tasks)):\n",
        "      tasks[i] = np.sort(tasks[i])\n",
        "  return tasks # ritorno gli indici per poi usare nella funzione successiva\n",
        "\n",
        "                                   #####################################################################################\n",
        "\n",
        "# METODO CHE ESEGUE IL TRAIN DELLA RETE                                   \n",
        "def train(trainloader,optimizer,criterion,scheduler,net):\n",
        "    losses = []\n",
        "    for epoch in range(20):  # loop over the dataset multiple times\n",
        "        start = timer()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            labels = data[1]\n",
        "            inputs, labels = data[0].to(device), labels.to(device) \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "        mean_losses= sum(losses)/len(losses)\n",
        "        #scheduler.step(mean_losses)\n",
        "        end = timer()\n",
        "        print(\"Tempo dell'epoca \",epoch,\"uguale a: \", (end - start), \"sec\")\n",
        "    print('ALLENAMENTO FINITO')\n",
        "    print(\"################################\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzr2ji4lUh1d"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "#device = \"cpu\"\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "ds_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                    download=True, transform=transform)\n",
        "ds_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                    download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7iqXeB-H12i"
      },
      "source": [
        "                    # NUOVI METODI IMPLEMENTATI: TEST GENERALE DELLA NET, TEST PER OGNI CLASSE, BACKBONE ROUTINE\n",
        "\n",
        "# METODO PER GENERARE I FILTERED DATASET CON L'OFFSET\n",
        "def task_dss(tasks,ds,offset_change=True): # aggiunto check su offset in modo tale da non cambaire offset ai task\n",
        "    task_dss = []\n",
        "    offset = 0\n",
        "    for tsk in tasks:\n",
        "      print(offset)\n",
        "      tmp = Filtered_Dataset(ds, tsk, offset)\n",
        "      task_dss.append(tmp)\n",
        "      if offset_change:\n",
        "          offset += len(tsk)\n",
        "      else :\n",
        "        offset = 0    \n",
        "    return task_dss  \n",
        "\n",
        "\n",
        "# METODO TASK SULLA RETE GENERALE    \n",
        "def test(testloader,net,task_dss,num_task):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1) # torna il valore max della tupla\n",
        "            if num_task <= 1:\n",
        "              for keys in task_dss.task2original:\n",
        "                  predicted[predicted == keys]=task_dss.task2original[keys]\n",
        "                  labels[labels == keys]=task_dss.task2original[keys]  \n",
        "            else:\n",
        "              for task in task_dss:\n",
        "                for keys in task.task2original:\n",
        "                  predicted[predicted == keys]=task.task2original[keys]\n",
        "                  labels[labels == keys]=task.task2original[keys]         \n",
        "            total += labels.size(0)\n",
        "            #print(total)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            #print('corrette',correct)\n",
        "    net.train()\n",
        "    print('Accuracy of the network : %d %%' % (\n",
        "            100 * correct / total)) \n",
        "    return (100 * correct / total)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqWz4tl9Vbys"
      },
      "source": [
        "def continual_table(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=True,agnsotic_test=True): \n",
        "  tsk = [] # indici dei task incrementali\n",
        "  accuracies1 = []\n",
        "  accuracies2 = [] \n",
        "  net = Net()\n",
        "  for i in range(0,len(tasks)):\n",
        "    index = i\n",
        "    print(net)\n",
        "    tsk.append(i)\n",
        "    net.add_task(len(tasks[i]))\n",
        "    if agnostic_train:\n",
        "       net.set_tasks(tsk)\n",
        "       net.to(device)\n",
        "       optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "       #optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
        "       criterion = nn.CrossEntropyLoss()\n",
        "       scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=4,verbose=True)\n",
        "       train(task_train_dls[i],optimizer,criterion,scheduler,net)\n",
        "       if agnsotic_test:\n",
        "         acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "         accuracies1.append(acc)\n",
        "       else:\n",
        "           net.set_tasks([i])  \n",
        "           net.to(device)  \n",
        "           acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "           accuracies1.append(acc)\n",
        "    else:\n",
        "      net.set_tasks([i])  \n",
        "      net.to(device) \n",
        "      optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "      #optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=4,verbose=True)\n",
        "      train(task_train_dls[i],optimizer,criterion,scheduler,net)\n",
        "      if agnsotic_test:\n",
        "          net.set_tasks(tsk)\n",
        "          net.to(device)\n",
        "          acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "          accuracies1.append(acc)\n",
        "      else:\n",
        "          net.set_tasks([i])  \n",
        "          net.to(device)  \n",
        "          acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "          accuracies1.append(acc)\n",
        "    print('TASK ',i)\n",
        "    if  index == (len(tasks)-1):\n",
        "      if agnsotic_test:\n",
        "          net.set_tasks(tsk)\n",
        "          net.to(device)   \n",
        "          print(tsk)\n",
        "          for idx in range (0,len(tsk)):\n",
        "            acc_tmp = test(task_test_dls[idx],net,task_test_dss[idx],num_task=1)\n",
        "            accuracies2.append(acc_tmp)\n",
        "      else:\n",
        "          print(tsk)\n",
        "          for idx in range (0,len(tsk)):\n",
        "            net.set_tasks([idx])\n",
        "            net.to(device)   \n",
        "            acc_tmp = test(task_test_dls[idx],net,task_test_dss[idx],num_task=1)\n",
        "            accuracies2.append(acc_tmp)\n",
        "  df = pd.DataFrame()\n",
        "  df['first_training']=accuracies1\n",
        "  df['last_training']=accuracies2 \n",
        "  avg1 = np.average(df['first_training'])\n",
        "  avg2 = np.average(df['last_training']) \n",
        "  print(\"Accuracy Precedente:\",avg1,\"Accuracy Successiva:\",avg2)\n",
        "  forgetting= avg1-avg2\n",
        "  print(\"Forgetting:\",forgetting)         \n",
        "  return net,df,forgetting\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdjq-YqaVYyy"
      },
      "source": [
        "# JOINT-TRAINING\n",
        "task_join = idx_tasks(1,ds_train,rand=False)\n",
        "task_train_dss = task_dss(task_join, ds_train,offset_change=True) \n",
        "task_test_dss = task_dss(task_join, ds_test,offset_change=True) \n",
        "task_train_dls = [DataLoader(ds,batch_size=16,shuffle=True,num_workers=4) for ds in task_train_dss]\n",
        "task_test_dls = [DataLoader(ds, batch_size=16,shuffle=False,num_workers=4) for ds in task_test_dss]\n",
        "\n",
        "print(task_join)\n",
        "trained_net_join,df_table_join, forgetting= continual_table(task_join,task_train_dls,task_test_dls,task_test_dss,agnostic_train=True,agnsotic_test=True)\n",
        "print(df_table_join)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kXCmMzJVetR"
      },
      "source": [
        "#Training_Aware\n",
        "tasks = idx_tasks(5,ds_train,rand=True) \n",
        "task_train_dss = task_dss(tasks, ds_train,offset_change=False) \n",
        "task_test_dss = task_dss(tasks, ds_test,offset_change=False) \n",
        "task_train_dls = [DataLoader(ds,batch_size=16,shuffle=True,num_workers=4) for ds in task_train_dss]\n",
        "task_test_dls = [DataLoader(ds, batch_size=16,shuffle=False,num_workers=4) for ds in task_test_dss]\n",
        "\n",
        "trained_net_aware,df_table_aw_aw, forgetting= continual_table(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=False,agnsotic_test=False)\n",
        "trained_net_aware,df_table_aw_agn, forgetting= continual_table(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=False,agnsotic_test=True)\n",
        "print(df_table_aw_aw)\n",
        "print(df_table_aw_agn) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSjo9hJlVfdg"
      },
      "source": [
        "#Training_Agnostic\n",
        "tasks = idx_tasks(5,ds_train,rand=False) \n",
        "task_train_dss = task_dss(tasks, ds_train,offset_change=True) \n",
        "task_test_dss = task_dss(tasks, ds_test,offset_change=False) \n",
        "task_train_dls = [DataLoader(ds,batch_size=16,shuffle=True,num_workers=4) for ds in task_train_dss]\n",
        "task_test_dls = [DataLoader(ds, batch_size=16,shuffle=False,num_workers=4) for ds in task_test_dss]\n",
        "\n",
        "trained_net_agnostic,df_table_agn_agn,forgetting = continual_table(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=True,agnsotic_test=True)\n",
        "trained_net_agnostic,df_table_agn_aw,forgetting = continual_table(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=True,agnsotic_test=False)\n",
        "\n",
        "print(df_table_agn_agn) \n",
        "print(df_table_agn_aw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA4sj1gP_Fee"
      },
      "source": [
        "# CLASSE IL DATASET FILTRATO REPLAY PER SOLUZIONE NAIVE\n",
        "class Filtered_Dataset_Replay(Dataset): \n",
        "  def __init__(self,dataset,indices,tasks,idx,offset=0,replay_num=100):\n",
        "        self.indices = indices # indici dei target di cui ci interessa il target\n",
        "        self.original_indices = [i for i in range(len(dataset.targets)) if dataset.targets[i] in indices] # filtraggio degli indici del dataset che voglio nel subset\n",
        "        if idx!=0:\n",
        "          for id in range(0,idx):\n",
        "           self.replay_indices = [i for i in range(len(dataset.targets)) if dataset.targets[i] in tasks[id]]\n",
        "           self.replay_indices = random.sample(self.replay_indices,replay_num)\n",
        "           self.original_indices = self.original_indices + self.replay_indices # concateno gli indici\n",
        "           self.indices = np.concatenate((tasks[id], self.indices), axis=None)\n",
        "           self.indices = np.arange(len(self.indices))\n",
        "        self.dataset = Subset(dataset, self.original_indices)\n",
        "        self.original2task = { self.indices[i]  : offset + i for i in range(0,  len(self.indices) ) } # remapping degli indici con chiavi i valori degli indici del del task\n",
        "        #self.original2task = self.dictmap(self.indices,offset)\n",
        "        self.task2original = dict({ (val, key) for (key, val) in self.original2task.items() }) # remapping degli indici con chiavi i valori degli indici del del task\n",
        "        \n",
        "  def __getitem__(self, idx):\n",
        "        (x, y) = self.dataset[idx]\n",
        "        return (x, self.original2task[y])\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.original_indices)\n",
        "\n",
        "  def dictmap(self,indices,offset):\n",
        "      original2task={}\n",
        "      idx = np.arange(len(indices))\n",
        "      print(idx)\n",
        "      idx_tmp = np.arange(len(indices))\n",
        "      for i in range(0,len(idx)):\n",
        "        if (idx[i]+offset) in indices:\n",
        "          original2task[idx[i]+offset] = (idx[i] + offset)\n",
        "          indices  = np.delete(indices, np.argwhere(indices == (idx[i]+offset)))\n",
        "          idx_tmp  = np.delete(idx_tmp,np.argwhere(idx_tmp == (idx[i])))     \n",
        "      for i in range(0,len(indices)):\n",
        "          original2task[indices[i]]=idx_tmp[i] + offset   \n",
        "      return original2task\n",
        "\n",
        "\n",
        "def task_dss_replay(tasks,ds,replay_num,offset_change=True): # aggiunto check su offset in modo tale da non cambaire offset ai task\n",
        "    task_dss = []\n",
        "    offset = 0\n",
        "    for i in range(0,len(tasks)) :\n",
        "      print(offset)\n",
        "      tmp = Filtered_Dataset_Replay(ds, tasks[i], tasks, i, offset,replay_num)\n",
        "      task_dss.append(tmp)\n",
        "      if offset_change:\n",
        "          offset += len(tasks[i])\n",
        "      else :\n",
        "        offset = 0    \n",
        "    return task_dss  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4e0wVO052ho"
      },
      "source": [
        "def continual_table_replay(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=True,agnsotic_test=True): \n",
        "  tsk = [] # indici dei task incrementali\n",
        "  accuracies1 = []\n",
        "  accuracies2 = [] \n",
        "  net = Net()\n",
        "  for i in range(0,len(tasks)):\n",
        "    index = i\n",
        "    print(net)\n",
        "    tsk.append(i)\n",
        "    net.add_task(len(tasks[i]))\n",
        "    if agnostic_train:\n",
        "       net.set_tasks(tsk)\n",
        "       net.to(device)\n",
        "       optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "       #optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
        "       criterion = nn.CrossEntropyLoss()\n",
        "       scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=4,verbose=True)\n",
        "       train(task_train_dls[i],optimizer,criterion,scheduler,net)\n",
        "       if agnsotic_test:\n",
        "         acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "         accuracies1.append(acc)\n",
        "       else:\n",
        "           net.set_tasks([i])  \n",
        "           net.to(device)  \n",
        "           acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "           accuracies1.append(acc)\n",
        "    else:\n",
        "      net.set_tasks([i])  \n",
        "      net.to(device) \n",
        "      optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "      #optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=4,verbose=True)\n",
        "      train(task_train_dls[i],optimizer,criterion,scheduler,net)\n",
        "      if agnsotic_test:\n",
        "          net.set_tasks(tsk)\n",
        "          net.to(device)\n",
        "          acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "          accuracies1.append(acc)\n",
        "      else:\n",
        "          net.set_tasks([i])  \n",
        "          net.to(device)  \n",
        "          acc = test(task_test_dls[i],net,task_test_dss[i],num_task=1)\n",
        "          accuracies1.append(acc)\n",
        "    print('TASK ',i)\n",
        "    if  index == (len(tasks)-1):\n",
        "      if agnsotic_test:\n",
        "          net.set_tasks(tsk)\n",
        "          net.to(device)   \n",
        "          print(tsk)\n",
        "          for idx in range (0,len(tsk)):\n",
        "            acc_tmp = test(task_test_dls[idx],net,task_test_dss[idx],num_task=1)\n",
        "            accuracies2.append(acc_tmp)\n",
        "      else:\n",
        "          print(tsk)\n",
        "          for idx in range (0,len(tsk)):\n",
        "            net.set_tasks([idx])\n",
        "            net.to(device)   \n",
        "            acc_tmp = test(task_test_dls[idx],net,task_test_dss[idx],num_task=1)\n",
        "            accuracies2.append(acc_tmp)\n",
        "  df = pd.DataFrame()\n",
        "  df['first_training']=accuracies1\n",
        "  df['last_training']=accuracies2 \n",
        "  avg1 = np.average(df['first_training'])\n",
        "  avg2 = np.average(df['last_training']) \n",
        "  print(\"Accuracy Precedente:\",avg1,\"Accuracy Successiva:\",avg2)\n",
        "  forgetting= avg1-avg2\n",
        "  print(\"Forgetting:\",forgetting)         \n",
        "  return net,df,avg1,avg2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip8lLYDPJhGv"
      },
      "source": [
        "#TRAINING AGNOSTIC REPLAY METHOD\n",
        "tasks = idx_tasks(5,ds_train,rand=False) \n",
        "task_test_dss = task_dss(tasks, ds_test,offset_change=False) \n",
        "acc1 = []\n",
        "acc2 = []\n",
        "\n",
        "for i in range(0,101,10):\n",
        "  task_train_dss_replay = task_dss_replay(tasks, ds_train,i,offset_change=False) \n",
        "  task_train_dls = [DataLoader(ds,batch_size=16,shuffle=True,num_workers=4) for ds in task_train_dss_replay]\n",
        "  task_test_dls = [DataLoader(ds, batch_size=16,shuffle=False,num_workers=4) for ds in task_test_dss]\n",
        "  trained_net_agnostic,df_table_agn_agn,avg_pre,avg_post = continual_table_replay(tasks,task_train_dls,task_test_dls,task_test_dss,agnostic_train=True,agnsotic_test=True)\n",
        "  acc1.append(avg_pre)\n",
        "  acc2.append(avg_post)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvPAE2SSDqqC"
      },
      "source": [
        "def plot_accuracies(df_agn,df_awa):\n",
        "  x = ['task 0', 'task 1', 'task 2', 'task 3', 'task 4']\n",
        "  y_joint = [77.6,77.6,77.6,77.6,77.6] # accuracy del joint training\n",
        "  y_first_agnostic = df_agn.first_training\n",
        "  y_first_aware = df_awa.first_training\n",
        "  y_last_agnostic = df_agn.last_training\n",
        "  y_last_aware = df_awa.last_training\n",
        "  plt.plot(x,y_joint,label='Joint Training')\n",
        "  plt.plot(x,y_first_agnostic,color='b',marker='o',label='First Train/Aware Test')\n",
        "  plt.plot(x,y_first_aware,color='r',marker='o',label='First Train/Agnostic Test')\n",
        "  plt.plot(x,y_last_agnostic,color='b',linestyle='--',marker='o',label='Last Train/Aware Test')\n",
        "  plt.plot(x,y_last_aware,color='r',linestyle='--',marker='o',label='Last Train/Agnostic Test')\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "  plt.xlabel('TASKS')\n",
        "  plt.ylabel('ACCURACIES')\n",
        "  plt.title('AGNOSTIC TRAINING')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-nITML7rfA"
      },
      "source": [
        "def plot_accuracies_replay(acc1,acc2):\n",
        "  x = ['0%','0,1%','0,2%','0,3%','0,4%','0,5%','0,6%','0,7%','0,8%','0,9%','1%'] # numero di esempi replay\n",
        "  y_joint = [77.6,77.6,77.6,77.6,77.6,77.6,77.6,77.6,77.6,77.6,77.6] # accuracy del joint-training\n",
        "  y_pre =np.array(acc1)\n",
        "  y_post = np.array(acc2)\n",
        "  y_pre = y_pre.astype(int)\n",
        "  y_post=y_post.astype(int)\n",
        "  plt.plot(x,y_joint,label='Joint Training')\n",
        "  plt.plot(x,y_pre,color='b',marker='o',label='First Training')\n",
        "  plt.plot(x,y_post,color='b',linestyle='--',marker='o',label='Last Training')\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "  plt.xlabel('PERCENTAGE OF REPLAY EXAMPLES')\n",
        "  plt.ylabel('ACCURACIES')\n",
        "  plt.title('AGNOSTIC REPLAY TRAINING')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}